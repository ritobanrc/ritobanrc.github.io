---
layout: post
title:  "Multimodal Natural Language Inference"
date:   2016-06-06 22:20:59 +00:00
image: /images/cs224u.png
categories: Stanford
author: "Leo Keselman"
subtitle: "CNNs meet RNNs"
paper: /pdfs/cs224u.pdf
course: "CS224U: Natural Language Understanding"
video: https://www.youtube.com/watch?v=WO1RJC_9k7s
---

We explored how natural language inference tasks can be augmented with visual data. 

Namely, we replicate and expand existing baselines for NLI, including recent deep learning methods. By adding image features to these models, we explore how the textual and visual modalities interact. Specifically, we show that image features can provide a small boost in classifier performance for simpler models, but are a subset of information provided in the premise statement and thus do not benefit complex models. Additionally, we demonstrate a weakness in the SNLI dataset, showing that textual entailment is predictable without reference to the premise statement. 

[CS224U Paper](/pdfs/cs224u.pdf){:target="_blank"}

[YouTube Presentation](https://www.youtube.com/watch?v=WO1RJC_9k7s){:target="_blank"}